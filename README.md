# YC-Go-Scraper# Job Scraper Engine



An intelligent job scraper written in Go that automatically finds early-career opportunities from companies using the Greenhouse platform.An intelligent job scraping system that automatically finds and filters job postings from company career pages and job boards.



## Features## ğŸ—ï¸ Project Structure



- ğŸ” **Smart Scraping**: Scrapes 40+ companies via Greenhouse API```

- ğŸ¯ **Intelligent Filtering**: Auto-filters for early-career roles (intern, new grad, junior)flight-code/

- ğŸš« **Exclusion Logic**: Removes senior/lead/principal positionsâ”œâ”€â”€ src/

- ğŸ—ºï¸ **Location Aware**: US locations onlyâ”‚   â”œâ”€â”€ scrapers/           # Job scraping modules

- ğŸ’¾ **PostgreSQL Storage**: Local database with deduplicationâ”‚   â”‚   â”œâ”€â”€ job_scraper_engine.py      # Main scraper engine

- ğŸ“Š **CSV Export**: Export to Excel-compatible formatâ”‚   â”‚   â”œâ”€â”€ debug_google_careers.py    # Google Careers debug tool

- ğŸ“ **Structured Logging**: Timestamped logs with levelsâ”‚   â”‚   â””â”€â”€ test_enhanced_scraper.py   # Scraper testing utility

- ğŸŒ **Web Dashboard**: Real-time job viewingâ”‚   â”œâ”€â”€ dashboard/          # Dashboard and tracking modules

- ğŸ³ **Docker Ready**: One-command deploymentâ”‚   â”‚   â”œâ”€â”€ dashboard_generator.py     # Dashboard generator

â”‚   â”‚   â”œâ”€â”€ job_tracker_v2.py         # Job tracking v2

## Quick Startâ”‚   â”‚   â””â”€â”€ job_tracker_v3.py         # Job tracking v3

â”‚   â””â”€â”€ utils/              # Utility scripts

```powershellâ”‚       â””â”€â”€ clear_db.py     # Database clearing utility

# Start everything (Postgres + Scraper + Dashboard)â”œâ”€â”€ config/                 # Configuration files

docker compose up -dâ”‚   â””â”€â”€ scraper_config.json # Main scraper configuration

â”œâ”€â”€ docs/                   # Documentation

# View scraper logsâ”‚   â”œâ”€â”€ README.md

docker logs yc-go-scraper-scraper-1 -fâ”‚   â”œâ”€â”€ SCRAPER_SETUP.md

â”‚   â”œâ”€â”€ DASHBOARD_SETUP.md

# Access dashboardâ”‚   â”œâ”€â”€ SETUP_GUIDE.md

# Open browser to: http://localhost:8080â”‚   â””â”€â”€ SETUP_GUIDE_V3.md

â”œâ”€â”€ data/                   # Data files and exports

# View CSV exportâ”‚   â”œâ”€â”€ job_applications.csv

Get-Content data/job_applications.csv | Select-Object -First 10â”‚   â””â”€â”€ google_careers_debug.html
# YC-Go-Scraper (Go-only)

An intelligent job scraper written in Go that discovers earlyâ€‘career roles from Greenhouse boards, stores them in Postgres, exports CSV, and serves a simple web dashboard.

## Features

- Smart scraping via Greenhouse API
- Earlyâ€‘career filtering (intern, new grad, junior) and USâ€‘location bias
- PostgreSQL storage with deâ€‘duplication on URL
- CSV export to `data/job_applications.csv`
- Web dashboard on http://localhost:8080
- Docker Compose for Postgres, scraper, and dashboard
- Structured logging with configurable level (LOG_LEVEL)
- JSON API: `/api/jobs` with pagination and status filter

## Quick start (Docker)

```powershell
# From repo root
docker compose up -d

# View scraper logs
docker logs yc-go-scraper-scraper-1 -f

# Open dashboard
start http://localhost:8080
```

CSV is written to `data/job_applications.csv` on the host.

## Configuration

Edit `config/scraper_config.json`:

```json
{
  "target_platforms": {
    "greenhouse": ["stripe", "airbnb", "coinbase", "databricks"]
  }
}
```

Environment variables:

- `POSTGRES_URL` (optional) override for database connection; defaults to local.
- `LOG_LEVEL` one of `DEBUG, INFO, WARN, ERROR` (default `INFO`).

## JSON API

The dashboard service also exposes a JSON endpoint for integrations:

- GET `/api/jobs?page=1&page_size=50&status=Applied|Not%20Applied`

Response shape:

```json
{
  "page": 1,
  "page_size": 50,
  "total": 382,
  "total_pages": 8,
  "items": [
    {
      "ID": 123,
      "Title": "Software Engineer",
      "Company": "Stripe",
      "Location": "San Francisco, CA, United States",
      "Type": "Engineering",
      "URL": "https://boards.greenhouse.io/...",
      "DateAdded": "2025-10-25T12:34:56Z",
      "Status": "Not Applied"
    }
  ]
}
```

## Local development

```powershell
# Start only Postgres
docker compose up -d db

# Run scraper (inside module)
cd go-scraper
go run ./cmd/scraper --config ../config/scraper_config.json

# Run dashboard
go run ./cmd/dashboard --port 8080

# Tests
go test ./...
```

## Project structure

```
YC-Go-Scraper/
â”œâ”€ go-scraper/
â”‚  â”œâ”€ cmd/
â”‚  â”‚  â”œâ”€ scraper/     # CLI scraper entrypoint
â”‚  â”‚  â””â”€ dashboard/   # Web dashboard entrypoint
â”‚  â”œâ”€ internal/
â”‚  â”‚  â”œâ”€ db/          # Postgres layer
â”‚  â”‚  â”œâ”€ scraper/     # Greenhouse scraper and filters
â”‚  â”‚  â”œâ”€ exporter/    # CSV exporter
â”‚  â”‚  â””â”€ logger/      # Structured logging
â”‚  â”œâ”€ Dockerfile
â”‚  â””â”€ Dockerfile.dashboard
â”œâ”€ config/
â”‚  â””â”€ scraper_config.json
â”œâ”€ data/               # CSV output folder
â””â”€ docker-compose.yml
```

## License

MIT