#
# GitHub Actions Workflow for Job Scraper Engine
# Runs the automated job discovery system on a schedule
#
name: Job Scraper Engine

# Controls when the action will run.
# 'workflow_dispatch' allows you to run it manually from the Actions tab.
# 'schedule' runs it automatically every 30 minutes during business hours
on:
  # workflow_dispatch:
  # schedule:
  #   # Run every 30 minutes from 9 AM to 6 PM UTC (business hours)
  #   - cron: '0,30 9-18 * * 1-5'  # Monday-Friday, 9 AM - 6 PM UTC

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out the repository
      - name: Checkout Repo
        uses: actions/checkout@v4

      # Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Install Chrome and dependencies
      - name: Install Chrome
        run: |
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list
          apt-get update
          apt-get install -y google-chrome-stable

      # Step 4: Install Python dependencies
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install supabase pandas google-generativeai selenium webdriver-manager beautifulsoup4 requests

      # Step 5: Run the job scraper engine
      - name: Run Job Scraper Engine
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: python job_scraper_engine.py

      # Step 6: Log results (optional)
      - name: Log Completion
        run: |
          echo "Job scraper engine completed at $(date)"
          echo "Check your Supabase database for new job postings" 
